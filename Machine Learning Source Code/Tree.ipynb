{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bd33e98-aef3-493f-b16a-59b124fad9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "{'petal length (cm)': {'<=2.45': '0', '>2.45': {'petal width (cm)': {'<=1.70': {'petal length (cm)': {'<=4.95': '1', '>4.95': '2'}}, '>1.70': {'petal length (cm)': {'<=4.80': '1', '>4.80': '2'}}}}}}\n",
      "\n",
      " prediction-truth Comparion:\n",
      "[[1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [0 0]] \n",
      "\n",
      " score:0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "class Tree:\n",
    "    max_depth=None\n",
    "    def __init__(self,max_depth):\n",
    "        self.max_depth=max_depth\n",
    "    def fit(self,x_train,y_train,feature_names):\n",
    "        '''\n",
    "        x_train should be 2-dimension array\n",
    "        y_train and feature_names should be 1-dimension array\n",
    "        \n",
    "        '''\n",
    "        self.x_train=x_train\n",
    "        self.y_train=y_train\n",
    "        self.feature_names=feature_names\n",
    "        def new_log(n,bottom):\n",
    "            '''\n",
    "            caculate log value with bottom,when n=0,log value=0.\n",
    "        \n",
    "            '''   \n",
    "            import math as m\n",
    "            if n==0:\n",
    "                return 0\n",
    "            else:\n",
    "                return m.log(n,bottom)\n",
    "\n",
    "        def continuous_encoder(x,border):\n",
    "            '''\n",
    "            encode continuous array into 2 catagories:'>border and '<=border'.\n",
    "        \n",
    "            '''\n",
    "            import numpy as np\n",
    "            n_values=x.shape[0]\n",
    "            new_x=[]\n",
    "            for k in range(n_values):\n",
    "                if x[k]>border:\n",
    "                    new_x.append(f'>{border:.2f}')\n",
    "                elif x[k]<=border:\n",
    "                    new_x.append(f'<={border:.2f}')\n",
    "            return np.array(new_x)\n",
    "        \n",
    "        def entropy(y):\n",
    "            '''\n",
    "            Caculate the entropy.  \n",
    "            y should be an 1-dimension array as target.\n",
    "            \n",
    "            '''\n",
    "            import numpy as np\n",
    "            catagories,counts=np.unique(y,return_counts=True)\n",
    "            sample_size=y.shape[0]\n",
    "            n_catagories=catagories.shape[0]\n",
    "            entropy=0\n",
    "            for i in range(n_catagories):\n",
    "                entropy-=(counts[i]/sample_size)*new_log(counts[i]/sample_size,2)\n",
    "            return entropy\n",
    "        \n",
    "        def info_gain(x,y):\n",
    "            '''\n",
    "            caculate the information gain.\n",
    "            x should be an 1-dimension array as a column of feature.\n",
    "            y should be an 1-dimension array as target.\n",
    "            \n",
    "            '''\n",
    "            import numpy as np\n",
    "            x_catagories,x_counts=np.unique(x,return_counts=True)\n",
    "            sample_size=x.shape[0]\n",
    "            n_x_catagories=x_catagories.shape[0]\n",
    "            weights=np.full(n_x_catagories,np.nan)\n",
    "            catagory_entropies=np.full(n_x_catagories,np.nan)\n",
    "            for i in range(n_x_catagories):\n",
    "                weights[i]=x_counts[i]/sample_size\n",
    "                catagory_entropies[i]=entropy(y[x==x_catagories[i]]) \n",
    "            info_gain= entropy(y)-np.sum(weights*catagory_entropies) \n",
    "            return info_gain\n",
    "        \n",
    "        def continuous_treatment(x,y):\n",
    "            '''\n",
    "            discretize an array that is thought to be continuous.\n",
    "            The output is the discretized array and the border that split the continious array calculated by max entropy.\n",
    "            \n",
    "            x,y should be 1-dimension array.\n",
    "            \n",
    "            '''\n",
    "            import numpy as np\n",
    "            n_type_values=np.unique(x).shape[0]\n",
    "            n_values=x.shape[0]\n",
    "            midpoint=np.full(n_values-1,np.nan)\n",
    "            info_gains=np.full(n_values-1,np.nan)\n",
    "            sorted_x=np.sort(x)\n",
    "            for i in range(n_values-1):\n",
    "                midpoint[i]=(sorted_x[i+1]-sorted_x[i])/2+sorted_x[i]\n",
    "                info_gains[i]=info_gain(continuous_encoder(x,midpoint[i]),y)\n",
    "            final_border=np.random.RandomState(123).choice(midpoint[info_gains==np.max(info_gains)])\n",
    "            return final_border,continuous_encoder(x,final_border)\n",
    "        \n",
    "        def best_feature(x_y,feature_names):\n",
    "            '''\n",
    "            Return the feature with max info_gain so that the dataframe can be splited next\n",
    "            x_y should be an 2-dimension array as features and target.\n",
    "        \n",
    "            '''\n",
    "            import numpy as np\n",
    "            x,y=np.hsplit(x_y,[x_y.shape[1]-1])\n",
    "            info_gains=np.full(x.shape[1],np.nan)\n",
    "            for i,featlist in enumerate(x.T):\n",
    "                if np.unique(x_train[:,i]).shape[0]>=5:\n",
    "                # if the types of feature in the initial dataframe>=5, the feature is thought to be continous,if continous, discretize it.\n",
    "                    info_gains[i]=info_gain(continuous_treatment(featlist,y)[1],y)\n",
    "                else:\n",
    "                    info_gains[i]=info_gain(featlist,y)\n",
    "            best_feature_index=np.random.RandomState(123).choice(np.arange(x.shape[1])[info_gains==np.max(info_gains)])\n",
    "            best_feature=feature_names[best_feature_index]\n",
    "            return best_feature,best_feature_index\n",
    "\n",
    "        def data_split(x_y,best_feature_index,catagory):\n",
    "            '''\n",
    "            split the dataframe according to the best_feature_indx,and the catagory of best_feature\n",
    "            \n",
    "            '''\n",
    "            import numpy as np\n",
    "            x,y=np.hsplit(x_y,[x_y.shape[1]-1])\n",
    "            best_featlist=x[:,best_feature_index]\n",
    "            if np.unique(x_train[:,best_feature_index]).shape[0]>=5:\n",
    "                border=continuous_treatment(best_featlist,y)[0]\n",
    "                if '<=' in catagory:\n",
    "                    splited_data=x_y[best_featlist<=border]\n",
    "                elif '>' in catagory:\n",
    "                    splited_data=x_y[best_featlist>border]\n",
    "            else:       \n",
    "                splited_data=x_y[best_featlist==catagory]            \n",
    "            return splited_data\n",
    "            \n",
    "        def new_int(a):\n",
    "            '''\n",
    "            used to the final prediction results,to make sure that the results are int if y contains number of str of number\n",
    "            and the results are text str if y contrains text str.\n",
    "\n",
    "            '''\n",
    "            try:\n",
    "                int(a)\n",
    "            except:\n",
    "                return a\n",
    "            else:\n",
    "                return int(a) \n",
    "                        \n",
    "        def create_tree(x_y,feature_names,max_depth=-1):\n",
    "            '''\n",
    "            create the tree in the form of dictionary\n",
    "\n",
    "            '''\n",
    "            import numpy as np\n",
    "            max_depth+=1\n",
    "            x,y=np.hsplit(x_y,[x_y.shape[1]-1])\n",
    "            if np.unique(y).shape[0]==1:\n",
    "                return str(new_int(np.unique(y)[0]))\n",
    "            best_feat,best_feat_index=best_feature(x_y,feature_names)\n",
    "            tree={best_feat:{}}\n",
    "            best_featlist=x[:,best_feat_index]\n",
    "            if np.unique(x_train[:,best_feat_index]).shape[0]>=5:\n",
    "                border=continuous_treatment(best_featlist,y)[0]\n",
    "                catagories=np.unique(continuous_encoder(best_featlist,border))\n",
    "            else:\n",
    "                catagories=np.unique(best_featlist)\n",
    "            if max_depth==self.max_depth:\n",
    "                class_list,class_count=np.unique(y,return_counts=True)\n",
    "                return str(new_int(np.random.RandomState(123).choice(class_list[class_count==np.max(class_count)])))\n",
    "            elif catagories.shape[0]==1:# When the dataframe can not be splited anymore\n",
    "                class_list,class_count=np.unique(y,return_counts=True)\n",
    "                return str(new_int(np.random.RandomState(123).choice(class_list[class_count==np.max(class_count)])))\n",
    "            else:\n",
    "                for catagory in catagories:\n",
    "                    tree[best_feat][catagory]=create_tree(\n",
    "                    data_split(x_y,best_feat_index,catagory),feature_names,max_depth=max_depth)\n",
    "                return tree\n",
    "  \n",
    "#  Encode data and treat continous data\n",
    "        import numpy as np\n",
    "        x_y=np.concatenate((x_train,y_train[:,np.newaxis]),axis=1) \n",
    "        tree=create_tree(x_y,feature_names)\n",
    "        self.tree=tree \n",
    "        \n",
    "    def predict(self,x_test):\n",
    "        '''\n",
    "        x_test should be a 2-dimension array\n",
    "\n",
    "        '''\n",
    "        def common(x):\n",
    "            '''\n",
    "            extract common part between the 2 elements in the 2-size list x\n",
    "\n",
    "            '''\n",
    "            result=float(x[0][2:])\n",
    "            return result\n",
    "            \n",
    "        def single_predict(tree,single_x_test):\n",
    "            '''\n",
    "            this function predict a test point, x_test should be a dictionary of a single data point\n",
    "            \n",
    "            '''\n",
    "            first=list(tree.keys())[0]\n",
    "            feature_index=np.arange(len(self.feature_names))[self.feature_names==first]\n",
    "            copy=single_x_test.copy()\n",
    "            if np.unique(self.x_train[:,feature_index].shape[0])>=5:\n",
    "                list_of_lists=list(tree[first].keys())\n",
    "                border=common(list_of_lists)                \n",
    "                if single_x_test[first]<=border:\n",
    "                    copy[first]=f'<={border:.2f}'\n",
    "                elif single_x_test[first]>border:\n",
    "                    copy[first]=f'>{border:.2f}' \n",
    "            second=copy[first]\n",
    "            edege=tree[first][second]\n",
    "            if isinstance(edege,str)==True:\n",
    "                predict=edege\n",
    "            else:\n",
    "                predict=single_predict(edege,single_x_test)\n",
    "            return predict\n",
    "        import numpy as np\n",
    "        empty_x_test=[]\n",
    "        for i,single_x_test in enumerate(x_test):\n",
    "            single_x_test=single_x_test.tolist()\n",
    "            single_x_test={feature_names:single_x_test for feature_names,single_x_test in zip(feature_names,single_x_test)}\n",
    "            empty_x_test.append(single_predict(self.tree,single_x_test))\n",
    "        return empty_x_test\n",
    "        \n",
    "#test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "x=load_iris().data\n",
    "y=load_iris().target\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=123)\n",
    "feature_names=np.array(load_iris().feature_names)\n",
    "clf=Tree(3)\n",
    "clf.fit(x_train,y_train,feature_names)\n",
    "y_pred=np.array(clf.predict(x_test))\n",
    "pred_y=np.concatenate((y_pred.astype(int)[:,np.newaxis],y_test[:,np.newaxis]),axis=1)\n",
    "score=np.unique(y_pred.astype(int)==y_test,return_counts=True)[1][1]/len(y_pred.astype(int)==y_test)\n",
    "print(f'Decision Tree:\\n{clf.tree}\\n\\n prediction-truth Comparion:\\n{pred_y} \\n\\n score:{score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea121b5-bc71-4bb3-a3c9-4f7c4d8d60a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
